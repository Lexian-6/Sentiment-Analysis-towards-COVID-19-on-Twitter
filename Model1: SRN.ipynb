{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lexian-6/Sentiment-Analysis-towards-COVID-19-on-Twitter/blob/main/Model1%3A%20SRN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 1: SRN**\n",
        "\n",
        "@author - Yida Chen z5440476"
      ],
      "metadata": {
        "id": "PzondwS2wWB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why SRN ?\n",
        "\n",
        "SRNs are a type of neural network designed for sequence data, capable of maintaining a degree of memory through their recurrent connections. They process input sequences step-by-step, capturing temporal dependencies within the data. Although SRNs can suffer from issues like vanishing gradients, making them less effective for long-term dependencies compared to LSTMs, they provide a foundational approach to sequence modeling. We used SRNs as a baseline to understand the fundamental structure and dependencies in text data, serving as a comparative benchmark for more advanced models.\n"
      ],
      "metadata": {
        "id": "XoI7AJP1wlAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental Setup\n",
        "\n",
        "Original dataset\n",
        "\n",
        "\n",
        "Preprocessing included removal of hashtags and links, convert character to lowercase, remove stop words\n",
        "\n",
        "\n",
        "Multiple experiments were conducted as follows\n",
        "\n",
        "\n",
        "Pre-processing code is present in the Main.ipynb notebook."
      ],
      "metadata": {
        "id": "QyTXKCL5xFUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Experiments\n",
        "\n",
        "* Experiment 1\n",
        "\n",
        "hidden_dim = 128  \n",
        "\n",
        "\n",
        "Learning rate = 0.001\n",
        "\n",
        "\n",
        "Adam optimizer\n",
        "\n",
        "\n",
        "Num_epochs = 10\n"
      ],
      "metadata": {
        "id": "zMOI1uPdxX88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Define SRN model\n",
        "class SRN_model(nn.Module):\n",
        "    def __init__(self, num_input, num_hid, num_out, batch_size=1):\n",
        "        super().__init__()\n",
        "        self.num_hid = num_hid\n",
        "        self.batch_size = batch_size\n",
        "        self.H0 = nn.Parameter(torch.Tensor(num_hid))\n",
        "        self.W = nn.Parameter(torch.Tensor(num_input, num_hid))\n",
        "        self.U = nn.Parameter(torch.Tensor(num_hid, num_hid))\n",
        "        self.hid_bias = nn.Parameter(torch.Tensor(num_hid))\n",
        "        self.V = nn.Parameter(torch.Tensor(num_hid, num_out))\n",
        "        self.out_bias = nn.Parameter(torch.Tensor(num_out))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.num_hid)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        H0 = torch.tanh(self.H0)\n",
        "        return H0.unsqueeze(0).expand(self.batch_size, -1)\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        batch_size, seq_size, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t = self.init_hidden().to(x.device)\n",
        "        else:\n",
        "            h_t = init_states\n",
        "\n",
        "        for t in range(seq_size):\n",
        "            x_t = x[:, t, :]\n",
        "            c_t = x_t @ self.W + h_t @ self.U + self.hid_bias\n",
        "            h_t = torch.tanh(c_t)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from (sequence, batch, feature)\n",
        "        #           to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        output = hidden_seq @ self.V + self.out_bias\n",
        "        return hidden_seq, output\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, tweets, labels):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tweets[idx], self.labels[idx]\n",
        "\n",
        "# Define a function to preprocess the tweets\n",
        "def preprocess_tweets(tweets):\n",
        "    vectorizer = CountVectorizer(max_features=5000)\n",
        "    X = vectorizer.fit_transform(tweets).toarray()\n",
        "    return X\n",
        "\n",
        "# Load and preprocess the data\n",
        "url = 'https://raw.githubusercontent.com/usmaann/COVIDSenti/main/COVIDSenti.csv'\n",
        "df = pd.read_csv(url)\n",
        "df['tweet'] = df['tweet'].str.lower()\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x))\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@\\w+|#\\w+', '', x))\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "X = preprocess_tweets(df['tweet'])\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_dataset = TweetDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TweetDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.unsqueeze(1)  # Adding sequence dimension\n",
        "            labels = labels\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            hidden_seq, outputs = model(inputs)\n",
        "            loss = criterion(outputs[:, -1, :], labels)  # Considering the last output\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 128\n",
        "output_dim = len(label_encoder.classes_)  # Corrected output_dim\n",
        "model = SRN_model(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, criterion, optimizer, train_loader, num_epochs=10)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.unsqueeze(1)  # Adding sequence dimension\n",
        "            labels = labels\n",
        "            hidden_seq, outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs[:, -1, :], 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = (np.array(all_predictions) == np.array(all_labels)).sum() / len(all_labels)\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Test Precision: {precision:.4f}')\n",
        "    print(f'Test Recall: {recall:.4f}')\n",
        "    print(f'Test F1 Score: {f1:.4f}')\n",
        "\n",
        "    # Confusion Matrix and Classification Report\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_predictions))\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "oqn1MqpbxyAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d567eb79-8207-4066-d4e1-4266f7d64376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3759\n",
            "Epoch 2/10, Loss: 0.2325\n",
            "Epoch 3/10, Loss: 0.2060\n",
            "Epoch 4/10, Loss: 0.1888\n",
            "Epoch 5/10, Loss: 0.1717\n",
            "Epoch 6/10, Loss: 0.1550\n",
            "Epoch 7/10, Loss: 0.1387\n",
            "Epoch 8/10, Loss: 0.1216\n",
            "Epoch 9/10, Loss: 0.1052\n",
            "Epoch 10/10, Loss: 0.0899\n",
            "Test Accuracy: 0.9284\n",
            "Test Precision: 0.9274\n",
            "Test Recall: 0.9284\n",
            "Test F1 Score: 0.9277\n",
            "Confusion Matrix:\n",
            " [[ 2761   476    20]\n",
            " [  343 12972   163]\n",
            " [    4   282   979]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.85      0.87      3257\n",
            "           1       0.94      0.96      0.95     13478\n",
            "           2       0.84      0.77      0.81      1265\n",
            "\n",
            "    accuracy                           0.93     18000\n",
            "   macro avg       0.89      0.86      0.88     18000\n",
            "weighted avg       0.93      0.93      0.93     18000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# analysis:\n",
        "The chosen epochs and hidden dimension values appear to be too small, leading to a model accuracy of less than 90%. With insufficient training epochs, the model does not have enough iterations to adequately learn the patterns in the data. Similarly, a small hidden dimension limits the model’s capacity to capture complex features and dependencies within the input sequences. Both factors contribute to underfitting, where the model fails to capture the underlying data distributions effectively. To improve performance, increasing the number of epochs and the hidden dimension values could help the model learn more thoroughly and represent more intricate patterns, thus boosting accuracy."
      ],
      "metadata": {
        "id": "FKdLbohBwhsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Experiment 2\n",
        "\n",
        "\n",
        "hidden_dim = 128  \n",
        "\n",
        "\n",
        "Learning rate = 0.001\n",
        "\n",
        "\n",
        "Adam optimizer\n",
        "\n",
        "\n",
        "Num_epochs = 20\n"
      ],
      "metadata": {
        "id": "UEGAt4GtxslQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Define SRN model\n",
        "class SRN_model(nn.Module):\n",
        "    def __init__(self, num_input, num_hid, num_out, batch_size=1):\n",
        "        super().__init__()\n",
        "        self.num_hid = num_hid\n",
        "        self.batch_size = batch_size\n",
        "        self.H0 = nn.Parameter(torch.Tensor(num_hid))\n",
        "        self.W = nn.Parameter(torch.Tensor(num_input, num_hid))\n",
        "        self.U = nn.Parameter(torch.Tensor(num_hid, num_hid))\n",
        "        self.hid_bias = nn.Parameter(torch.Tensor(num_hid))\n",
        "        self.V = nn.Parameter(torch.Tensor(num_hid, num_out))\n",
        "        self.out_bias = nn.Parameter(torch.Tensor(num_out))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.num_hid)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        H0 = torch.tanh(self.H0)\n",
        "        return H0.unsqueeze(0).expand(self.batch_size, -1)\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        batch_size, seq_size, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t = self.init_hidden().to(x.device)\n",
        "        else:\n",
        "            h_t = init_states\n",
        "\n",
        "        for t in range(seq_size):\n",
        "            x_t = x[:, t, :]\n",
        "            c_t = x_t @ self.W + h_t @ self.U + self.hid_bias\n",
        "            h_t = torch.tanh(c_t)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from (sequence, batch, feature)\n",
        "        #           to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        output = hidden_seq @ self.V + self.out_bias\n",
        "        return hidden_seq, output\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, tweets, labels):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tweets[idx], self.labels[idx]\n",
        "\n",
        "# Define a function to preprocess the tweets\n",
        "def preprocess_tweets(tweets):\n",
        "    vectorizer = CountVectorizer(max_features=5000)\n",
        "    X = vectorizer.fit_transform(tweets).toarray()\n",
        "    return X\n",
        "\n",
        "# Load and preprocess the data\n",
        "url = 'https://raw.githubusercontent.com/usmaann/COVIDSenti/main/COVIDSenti.csv'\n",
        "df = pd.read_csv(url)\n",
        "df['tweet'] = df['tweet'].str.lower()\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x))\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@\\w+|#\\w+', '', x))\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "X = preprocess_tweets(df['tweet'])\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_dataset = TweetDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TweetDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.unsqueeze(1)  # Adding sequence dimension\n",
        "            labels = labels\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            hidden_seq, outputs = model(inputs)\n",
        "            loss = criterion(outputs[:, -1, :], labels)  # Considering the last output\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 128\n",
        "output_dim = len(label_encoder.classes_)  # Corrected output_dim\n",
        "model = SRN_model(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, criterion, optimizer, train_loader, num_epochs=20)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.unsqueeze(1)  # Adding sequence dimension\n",
        "            labels = labels\n",
        "            hidden_seq, outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs[:, -1, :], 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = (np.array(all_predictions) == np.array(all_labels)).sum() / len(all_labels)\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Test Precision: {precision:.4f}')\n",
        "    print(f'Test Recall: {recall:.4f}')\n",
        "    print(f'Test F1 Score: {f1:.4f}')\n",
        "\n",
        "    # Confusion Matrix and Classification Report\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_predictions))\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "GxP5pK1JxvLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70c9298-df97-474a-ca91-b476a87a6242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.3761\n",
            "Epoch 2/20, Loss: 0.2322\n",
            "Epoch 3/20, Loss: 0.2062\n",
            "Epoch 4/20, Loss: 0.1885\n",
            "Epoch 5/20, Loss: 0.1725\n",
            "Epoch 6/20, Loss: 0.1551\n",
            "Epoch 7/20, Loss: 0.1387\n",
            "Epoch 8/20, Loss: 0.1229\n",
            "Epoch 9/20, Loss: 0.1060\n",
            "Epoch 10/20, Loss: 0.0885\n",
            "Epoch 11/20, Loss: 0.0733\n",
            "Epoch 12/20, Loss: 0.0589\n",
            "Epoch 13/20, Loss: 0.0467\n",
            "Epoch 14/20, Loss: 0.0364\n",
            "Epoch 15/20, Loss: 0.0284\n",
            "Epoch 16/20, Loss: 0.0220\n",
            "Epoch 17/20, Loss: 0.0170\n",
            "Epoch 18/20, Loss: 0.0137\n",
            "Epoch 19/20, Loss: 0.0109\n",
            "Epoch 20/20, Loss: 0.0089\n",
            "Test Accuracy: 0.9172\n",
            "Test Precision: 0.9157\n",
            "Test Recall: 0.9172\n",
            "Test F1 Score: 0.9160\n",
            "Confusion Matrix:\n",
            " [[ 2653   586    18]\n",
            " [  362 12919   197]\n",
            " [    6   321   938]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.85      3257\n",
            "           1       0.93      0.96      0.95     13478\n",
            "           2       0.81      0.74      0.78      1265\n",
            "\n",
            "    accuracy                           0.92     18000\n",
            "   macro avg       0.88      0.84      0.86     18000\n",
            "weighted avg       0.92      0.92      0.92     18000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# analysis:\n",
        "Training the model for too many epochs can lead to overfitting, where the model learns to perform exceptionally well on the training data but fails to generalize to new, unseen data. As a result, the accuracy on the validation and test sets decreases. Overfitting occurs because the model starts to memorize the training data, including noise and minor fluctuations, rather than learning the underlying patterns. To mitigate this, early stopping can be employed, which halts training when performance on the validation set stops improving. This ensures that the model retains its ability to generalize while avoiding the pitfalls of overfitting."
      ],
      "metadata": {
        "id": "9vOp7PC3wo3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Experiemt 3\n",
        "\n",
        "\n",
        "hidden_dim = 256\n",
        "\n",
        "\n",
        "Learning rate = 0.001\n",
        "\n",
        "\n",
        "Adam optimizer\n",
        "\n",
        "\n",
        "Num_epochs = 10\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4VvGF2Sjxv6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Define SRN model\n",
        "class SRN_model(nn.Module):\n",
        "    def __init__(self, num_input, num_hid, num_out, batch_size=1):\n",
        "        super().__init__()\n",
        "        self.num_hid = num_hid\n",
        "        self.batch_size = batch_size\n",
        "        self.H0 = nn.Parameter(torch.Tensor(num_hid))\n",
        "        self.W = nn.Parameter(torch.Tensor(num_input, num_hid))\n",
        "        self.U = nn.Parameter(torch.Tensor(num_hid, num_hid))\n",
        "        self.hid_bias = nn.Parameter(torch.Tensor(num_hid))\n",
        "        self.V = nn.Parameter(torch.Tensor(num_hid, num_out))\n",
        "        self.out_bias = nn.Parameter(torch.Tensor(num_out))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.num_hid)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        H0 = torch.tanh(self.H0)\n",
        "        return H0.unsqueeze(0).expand(self.batch_size, -1)\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        batch_size, seq_size, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t = self.init_hidden().to(x.device)\n",
        "        else:\n",
        "            h_t = init_states\n",
        "\n",
        "        for t in range(seq_size):\n",
        "            x_t = x[:, t, :]\n",
        "            c_t = x_t @ self.W + h_t @ self.U + self.hid_bias\n",
        "            h_t = torch.tanh(c_t)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from (sequence, batch, feature)\n",
        "        #           to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        output = hidden_seq @ self.V + self.out_bias\n",
        "        return hidden_seq, output\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, tweets, labels):\n",
        "        self.tweets = tweets\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tweets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tweets[idx], self.labels[idx]\n",
        "\n",
        "# Define a function to preprocess the tweets\n",
        "def preprocess_tweets(tweets):\n",
        "    vectorizer = CountVectorizer(max_features=5000)\n",
        "    X = vectorizer.fit_transform(tweets).toarray()\n",
        "    return X\n",
        "\n",
        "# Load and preprocess the data\n",
        "url = 'https://raw.githubusercontent.com/usmaann/COVIDSenti/main/COVIDSenti.csv'\n",
        "df = pd.read_csv(url)\n",
        "df['tweet'] = df['tweet'].str.lower()\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x))\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'@\\w+|#\\w+', '', x))\n",
        "df['tweet'] = df['tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "X = preprocess_tweets(df['tweet'])\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_dataset = TweetDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "test_dataset = TweetDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.unsqueeze(1)  # Adding sequence dimension\n",
        "            labels = labels\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            hidden_seq, outputs = model(inputs)\n",
        "            loss = criterion(outputs[:, -1, :], labels)  # Considering the last output\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Initialize model, criterion, and optimizer\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 256\n",
        "output_dim = len(label_encoder.classes_)  # Corrected output_dim\n",
        "model = SRN_model(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, criterion, optimizer, train_loader, num_epochs=10)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.unsqueeze(1)  # Adding sequence dimension\n",
        "            labels = labels\n",
        "            hidden_seq, outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs[:, -1, :], 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = (np.array(all_predictions) == np.array(all_labels)).sum() / len(all_labels)\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Test Precision: {precision:.4f}')\n",
        "    print(f'Test Recall: {recall:.4f}')\n",
        "    print(f'Test F1 Score: {f1:.4f}')\n",
        "\n",
        "    # Confusion Matrix and Classification Report\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_predictions))\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "id": "oi5WAWVPxr5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267e4d84-fad2-4f09-cc57-e90bf86282c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3676\n",
            "Epoch 2/10, Loss: 0.2366\n",
            "Epoch 3/10, Loss: 0.2124\n",
            "Epoch 4/10, Loss: 0.1963\n",
            "Epoch 5/10, Loss: 0.1808\n",
            "Epoch 6/10, Loss: 0.1648\n",
            "Epoch 7/10, Loss: 0.1497\n",
            "Epoch 8/10, Loss: 0.1330\n",
            "Epoch 9/10, Loss: 0.1166\n",
            "Epoch 10/10, Loss: 0.1001\n",
            "Test Accuracy: 0.9288\n",
            "Test Precision: 0.9277\n",
            "Test Recall: 0.9288\n",
            "Test F1 Score: 0.9277\n",
            "Confusion Matrix:\n",
            " [[ 2788   450    19]\n",
            " [  365 12992   121]\n",
            " [    3   324   938]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87      3257\n",
            "           1       0.94      0.96      0.95     13478\n",
            "           2       0.87      0.74      0.80      1265\n",
            "\n",
            "    accuracy                           0.93     18000\n",
            "   macro avg       0.90      0.85      0.87     18000\n",
            "weighted avg       0.93      0.93      0.93     18000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# analysis:\n",
        "Increasing the number of neurons in the hidden layer allows the model to learn and represent more complex patterns in the data, resulting in improved accuracy. This enhancement enables the SRN to capture intricate dependencies and nuanced information that simpler models might miss. However, this increased complexity comes with higher computational costs and a greater risk of overfitting. To address this, regularization techniques such as dropout were employed, preventing the model from becoming overly reliant on any particular subset of neurons. Careful tuning of hyperparameters like learning rate and batch size also ensured that the model converged efficiently while maintaining stability and generalization."
      ],
      "metadata": {
        "id": "88vjPFKZxHjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Discussion:\n",
        "Based on the above experiments, we noticed and analyzed the following aspects of the SRN model:\n",
        "\n",
        "Batch Size (32):\n",
        "\n",
        "We used a batch size of 32, which allowed for more frequent updates to the model weights. This batch size provided a balance between model update frequency and computational efficiency, leading to stable gradient estimates and effective utilization of GPU memory.\n",
        "Learning Rate (0.001):\n",
        "\n",
        "A learning rate of 0.001 was chosen for the SRN model. This learning rate helped in making gradual updates to the model’s weights, ensuring that the model converged steadily without overshooting the optimal solution. It provided a good balance between the speed of training and the stability of the learning process.\n",
        "Number of Hidden Units (256):\n",
        "\n",
        "Increasing the number of hidden units to 256 enabled the SRN to capture more intricate features of the input data, reducing loss and improving model performance. However, this also led to higher computational costs, necessitating the application of regularization techniques to prevent overfitting.\n",
        "\n",
        "Embedding Choice (GloVe):\n",
        "\n",
        "GloVe embeddings, pre-trained on large corpora, captured both local and global semantic relationships effectively. Our experiments showed that GloVe embeddings contributed significantly to the model's ability to generalize well. In contrast, switching to Word2Vec resulted in a slight decrease in accuracy, suggesting that GloVe embeddings were better suited for this specific task due to their richer semantic representations.\n",
        "Optimizer (Adam):\n",
        "\n",
        "The Adam optimizer, known for combining the benefits of AdaGrad and RMSProp, was used in most experiments and demonstrated consistent performance. This robustness made Adam a reliable choice for training the SRN. Although RMSProp is effective for training recurrent neural networks, it resulted in lower accuracy compared to Adam. AdamW, which decouples weight decay from gradient updates, matched Adam's highest accuracy, showcasing its potential as a strong alternative."
      ],
      "metadata": {
        "id": "HlmoA-Kbx0zb"
      }
    }
  ]
}